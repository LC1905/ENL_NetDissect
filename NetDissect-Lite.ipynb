{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "from torch.autograd import Variable as V\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "from functools import partial\n",
    "import re\n",
    "import random\n",
    "import signal\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "from scipy.misc import imread\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import torchvision\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "24e3ebd2-2311-436d-8504-776c516f9fe0"
    }
   },
   "source": [
    "### 2. set global varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "1691fa8a-c877-4024-99ac-883e200d2d96"
    }
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "######### global settings  #########\n",
    "GPU = True                                  # running on GPU is highly suggested\n",
    "TEST_MODE = False                           # turning on the testmode means the code will run on a small dataset.\n",
    "CLEAN = True                               # set to \"True\" if you want to clean the temporary large files after generating result\n",
    "MODEL = 'resnet18'                          # model arch: resnet18, alexnet, resnet50, densenet161\n",
    "DATASET = 'places365'                       # model trained on: places365 or imagenet\n",
    "QUANTILE = 0.005                            # the threshold used for activation\n",
    "SEG_THRESHOLD = 0.04                        # the threshold used for visualization\n",
    "SCORE_THRESHOLD = 0.04                      # the threshold used for IoU score (in HTML file)\n",
    "TOPN = 10                                   # to show top N image with highest activation for each unit\n",
    "PARALLEL = 1                                # how many process is used for tallying (Experiments show that 1 is the fastest)\n",
    "CATAGORIES = [\"object\", \"part\",\"scene\",\"texture\",\"color\"] # concept categories that are chosen to detect: \"object\", \"part\", \"scene\", \"material\", \"texture\", \"color\"\n",
    "OUTPUT_FOLDER = \"../../../project2/bermanm/netdissect/dissection/\" + MODEL + \"_\" + DATASET # result will be stored in this folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. set some model-dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### sub settings ###########\n",
    "\n",
    "# set data set and image size\n",
    "if MODEL != 'alexnet': # if the model is not alexnet, use broden1_224\n",
    "    DATA_DIRECTORY = '../../../project/rcc/deep_learning_hack/netdissect/broden1_224'\n",
    "    IMG_SIZE = 224\n",
    "else: # otherwise use broden1_227\n",
    "    DATA_DIRECTORY = '../../../project/rcc/deep_learning_hack/netdissect/broden1_227'\n",
    "    IMG_SIZE = 227\n",
    "\n",
    "# set number of classes\n",
    "if DATASET == 'places365':\n",
    "    NUM_CLASSES = 365\n",
    "elif DATASET == 'imagenet':\n",
    "    NUM_CLASSES = 1000\n",
    "    \n",
    "if MODEL == 'resnet18':\n",
    "    FEATURE_NAMES = ['layer4']\n",
    "    if DATASET == 'places365':\n",
    "        MODEL_FILE = '/home/canliu/NetDissect/zoo/resnet18_places365.pth.tar' # model data is stored here\n",
    "        MODEL_PARALLEL = True\n",
    "    elif DATASET == 'imagenet':\n",
    "        MODEL_FILE = None\n",
    "        MODEL_PARALLEL = False\n",
    "elif MODEL == 'densenet161': # cannot run it now\n",
    "    FEATURE_NAMES = ['features']\n",
    "    if DATASET == 'places365':\n",
    "        MODEL_FILE = 'zoo/whole_densenet161_places365_python36.pth.tar'\n",
    "        MODEL_PARALLEL = False\n",
    "elif MODEL == 'resnet50': # cannot run it now\n",
    "    FEATURE_NAMES = ['layer4']\n",
    "    if DATASET == 'places365':\n",
    "        MODEL_FILE = 'zoo/whole_resnet50_places365_python36.pth.tar'\n",
    "        MODEL_PARALLEL = False\n",
    "\n",
    "# set parameters depending on whether it is test mod\n",
    "if TEST_MODE:\n",
    "    WORKERS = 1\n",
    "    BATCH_SIZE = 4\n",
    "    TALLY_BATCH_SIZE = 2\n",
    "    TALLY_AHEAD = 1\n",
    "    INDEX_FILE = 'index_sm.csv' # what is this file?\n",
    "    OUTPUT_FOLDER += \"_test\"\n",
    "else:\n",
    "    WORKERS = 12\n",
    "    BATCH_SIZE = 128\n",
    "    TALLY_BATCH_SIZE = 16\n",
    "    TALLY_AHEAD = 4\n",
    "    INDEX_FILE = 'index.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. helper - decode_index_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_index_dict(row):\n",
    "    result = {}\n",
    "    for key, val in row.items():\n",
    "        if key in ['image', 'split']:\n",
    "            result[key] = val\n",
    "        elif key in ['sw', 'sh', 'iw', 'ih']:\n",
    "            result[key] = int(val)\n",
    "        else:\n",
    "            item = [s for s in val.split(';') if s]\n",
    "            for i, v in enumerate(item):\n",
    "                if re.match('^\\d+$', v):\n",
    "                    item[i] = int(v)\n",
    "            result[key] = item\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. helper -  decode_label_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_label_dict(row):\n",
    "    result = {}\n",
    "    for key, val in row.items():\n",
    "        if key == 'category':\n",
    "            result[key] = dict((c, int(n))\n",
    "                for c, n in [re.match('^([^(]*)\\(([^)]*)\\)$', f).groups()\n",
    "                    for f in val.split(';')])\n",
    "        elif key == 'name':\n",
    "            result[key] = val\n",
    "        elif key == 'syns':\n",
    "            result[key] = val.split(';')\n",
    "        elif re.match('^\\d+$', val):\n",
    "            result[key] = int(val)\n",
    "        elif re.match('^\\d+\\.\\d*$', val):\n",
    "            result[key] = float(val)\n",
    "        else:\n",
    "            result[key] = val\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. helper - index_has_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_has_all_data(row, categories):\n",
    "    for c in categories:\n",
    "        cat_has = False\n",
    "        for data in row[c]:\n",
    "            if data:\n",
    "                cat_has = True\n",
    "                break\n",
    "        if not cat_has:\n",
    "            return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. helper - index_has_any_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_has_any_data(row, categories):\n",
    "    for c in categories:\n",
    "        for data in row[c]:\n",
    "            if data: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. helper - build_numpy_category_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_numpy_category_map(map_data, key1='code', key2='number'):\n",
    "    '''\n",
    "    Input: set of rows with 'number' fields (or another field name key).\n",
    "    Output: array such that a[number] = the row with the given number.\n",
    "    '''\n",
    "    results = list(numpy.zeros((max([d[key] for d in map_data]) + 1),\n",
    "            dtype=numpy.int16) for key in (key1, key2))\n",
    "    for d in map_data:\n",
    "        results[0][d[key1]] = d[key2]\n",
    "        results[1][d[key2]] = d[key1]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. load AbstractSegmentation class\n",
    "    HACK IT LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractSegmentation:\n",
    "    def all_names(self, category, j):\n",
    "        raise NotImplementedError\n",
    "    def size(self, split=None):\n",
    "        return 0\n",
    "    def filename(self, i):\n",
    "        raise NotImplementedError\n",
    "    def metadata(self, i):\n",
    "        return self.filename(i)\n",
    "    @classmethod\n",
    "    def resolve_segmentation(cls, m):\n",
    "        return {}\n",
    "\n",
    "    def name(self, category, i):\n",
    "        '''\n",
    "        Default implemtnation for segmentation_data,\n",
    "        utilizing all_names.\n",
    "        '''\n",
    "        all_names = self.all_names(category, i)\n",
    "        return all_names[0] if len(all_names) else ''\n",
    "\n",
    "    def segmentation_data(self, category, i, c=0, full=False):\n",
    "        '''\n",
    "        Default implemtnation for segmentation_data,\n",
    "        utilizing metadata and resolve_segmentation.\n",
    "        '''\n",
    "        segs = self.resolve_segmentation(\n",
    "                self.metadata(i), categories=[category])\n",
    "        if category not in segs:\n",
    "            return 0\n",
    "        data = segs[category]\n",
    "        if not full and len(data.shape) >= 3:\n",
    "            return data[0]\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Load SegmentationData Class\n",
    "    HACK IT LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationData(AbstractSegmentation):\n",
    "    '''\n",
    "    Represents and loads a multi-channel segmentation represented with\n",
    "    a series of csv files: index.csv lists the images together with\n",
    "    any label data avilable in each category; category.csv lists\n",
    "    the categories of segmentations available; and label.csv lists the\n",
    "    numbers used to describe each label class. In addition, the categories\n",
    "    each have a separate c_*.csv file describing a dense coding of labels.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, directory, categories=None, require_all=False):\n",
    "        directory = os.path.expanduser(directory)\n",
    "        self.directory = directory\n",
    "        with open(os.path.join(directory, INDEX_FILE)) as f:\n",
    "            self.image = [decode_index_dict(r) for r in csv.DictReader(f)]\n",
    "        with open(os.path.join(directory, 'category.csv')) as f:\n",
    "            self.category = OrderedDict()\n",
    "            for row in csv.DictReader(f):\n",
    "                if categories and row['name'] in categories:\n",
    "                    self.category[row['name']] = row\n",
    "        categories = self.category.keys()\n",
    "        with open(os.path.join(directory, 'label.csv')) as f:\n",
    "            label_data = [decode_label_dict(r) for r in csv.DictReader(f)]\n",
    "        self.label = build_dense_label_array(label_data)\n",
    "        # Filter out images with insufficient data\n",
    "        filter_fn = partial(\n",
    "                index_has_all_data if require_all else index_has_any_data,\n",
    "                categories=categories)\n",
    "        self.image = [row for row in self.image if filter_fn(row)]\n",
    "        # Build dense remapping arrays for labels, so that you can\n",
    "        # get dense ranges of labels for each category.\n",
    "        self.category_map = {}\n",
    "        self.category_unmap = {}\n",
    "        self.category_label = {}\n",
    "        for cat in self.category:\n",
    "            with open(os.path.join(directory, 'c_%s.csv' % cat)) as f:\n",
    "                c_data = [decode_label_dict(r) for r in csv.DictReader(f)]\n",
    "            self.category_unmap[cat], self.category_map[cat] = (\n",
    "                    build_numpy_category_map(c_data))\n",
    "            self.category_label[cat] = build_dense_label_array(\n",
    "                    c_data, key='code')\n",
    "\n",
    "        self.labelcat = self.onehot(self.primary_categories_per_index())\n",
    "\n",
    "    def primary_categories_per_index(ds):\n",
    "        '''\n",
    "        Returns an array of primary category numbers for each label, where the\n",
    "        first category listed in ds.category_names is given category number 0.\n",
    "        '''\n",
    "        catmap = {}\n",
    "        categories = ds.category_names()\n",
    "        for cat in categories:\n",
    "            imap = ds.category_index_map(cat)\n",
    "            if len(imap) < ds.label_size(None):\n",
    "                imap = np.concatenate((imap, np.zeros(\n",
    "                    ds.label_size(None) - len(imap), dtype=imap.dtype)))\n",
    "            catmap[cat] = imap\n",
    "        result = []\n",
    "        for i in range(ds.label_size(None)):\n",
    "            maxcov, maxcat = max(\n",
    "                (ds.coverage(cat, catmap[cat][i]) if catmap[cat][i] else 0, ic)\n",
    "                for ic, cat in enumerate(categories))\n",
    "            result.append(maxcat)\n",
    "        return np.array(result)\n",
    "\n",
    "    def onehot(self, arr, minlength=None):\n",
    "        '''\n",
    "        Expands an array of integers in one-hot encoding by adding a new last\n",
    "        dimension, leaving zeros everywhere except for the nth dimension, where\n",
    "        the original array contained the integer n.  The minlength parameter is\n",
    "        used to indcate the minimum size of the new dimension.\n",
    "        '''\n",
    "        length = np.amax(arr) + 1\n",
    "        if minlength is not None:\n",
    "            length = max(minlength, length)\n",
    "        result = np.zeros(arr.shape + (length,))\n",
    "        result[list(np.indices(arr.shape)) + [arr]] = 1\n",
    "        return result\n",
    "\n",
    "\n",
    "    def all_names(self, category, j):\n",
    "        '''All English synonyms for the given label'''\n",
    "        if category is not None:\n",
    "            j = self.category_unmap[category][j]\n",
    "        return [self.label[j]['name']] + self.label[j]['syns']\n",
    "\n",
    "    def size(self, split=None):\n",
    "        '''The number of images in this data set.'''\n",
    "        if split is None:\n",
    "            return len(self.image)\n",
    "        return len([im for im in self.image if im['split'] == split])\n",
    "\n",
    "    def filename(self, i):\n",
    "        '''The filename of the ith jpeg (original image).'''\n",
    "        return os.path.join(self.directory, 'images', self.image[i]['image'])\n",
    "\n",
    "    def split(self, i):\n",
    "        '''Which split contains item i.'''\n",
    "        return self.image[i]['split']\n",
    "\n",
    "    def metadata(self, i):\n",
    "        '''Extract metadata for image i, For efficient data loading.'''\n",
    "        return self.directory, self.image[i]\n",
    "\n",
    "    meta_categories = ['image', 'split', 'ih', 'iw', 'sh', 'sw']\n",
    "\n",
    "    @classmethod\n",
    "    def resolve_segmentation(cls, m, categories=None):\n",
    "        '''\n",
    "        Resolves a full segmentation, potentially in a differenct process,\n",
    "        for efficient multiprocess data loading.\n",
    "        '''\n",
    "        directory, row = m\n",
    "        result = {}\n",
    "        for cat, d in row.items():\n",
    "            if cat in cls.meta_categories:\n",
    "                continue\n",
    "            if not wants(cat, categories):\n",
    "                continue\n",
    "            if all(isinstance(data, int) for data in d):\n",
    "                result[cat] = d\n",
    "                continue\n",
    "            out = numpy.empty((len(d), row['sh'], row['sw']), dtype=numpy.int16)\n",
    "            for i, channel in enumerate(d):\n",
    "                if isinstance(channel, int):\n",
    "                    out[i] = channel\n",
    "                else:\n",
    "                    rgb = imread(os.path.join(directory, 'images', channel))\n",
    "                    out[i] = rgb[:,:,0] + rgb[:,:,1] * 256\n",
    "            result[cat] = out\n",
    "        return result, (row['sh'], row['sw'])\n",
    "\n",
    "    def label_size(self, category=None):\n",
    "        '''\n",
    "        Returns the number of distinct labels (plus zero), i.e., one\n",
    "        more than the maximum label number.  If a category is specified,\n",
    "        returns the number of distinct labels within that category.\n",
    "        '''\n",
    "        if category is None:\n",
    "            return len(self.label)\n",
    "        else:\n",
    "            return len(self.category_unmap[category])\n",
    "\n",
    "    def name(self, category, j):\n",
    "        '''\n",
    "        Returns an English name for the jth label.  If a category is\n",
    "        specified, returns the name for the category-specific nubmer j.\n",
    "        If category=None, then treats j as a fully unified index number.\n",
    "        '''\n",
    "        if category is not None:\n",
    "            j = self.category_unmap[category][j]\n",
    "        return self.label[j]['name']\n",
    "\n",
    "    def frequency(self, category, j):\n",
    "        '''\n",
    "        Returns the number of images for which the label appears.\n",
    "        '''\n",
    "        if category is not None:\n",
    "            return self.category_label[category][j]['frequency']\n",
    "        return self.label[j]['frequency']\n",
    "\n",
    "    def coverage(self, category, j):\n",
    "        '''\n",
    "        Returns the pixel coverage of the label in units of whole-images.\n",
    "        '''\n",
    "        if category is not None:\n",
    "            return self.category_label[category][j]['coverage']\n",
    "        return self.label[j]['coverage']\n",
    "\n",
    "    def category_names(self):\n",
    "        '''\n",
    "        Returns the set of category names.\n",
    "        '''\n",
    "        return list(self.category.keys())\n",
    "\n",
    "    def category_frequency(self, category):\n",
    "        '''\n",
    "        Returns the number of images touched by a category.\n",
    "        '''\n",
    "        return float(self.category[category]['frequency'])\n",
    "\n",
    "    def primary_categories_per_index(self, categories=None):\n",
    "        '''\n",
    "        Returns an array of primary category numbers for each label, where\n",
    "        catagories are indexed according to the list of categories passed,\n",
    "        or self.category_names() if none.\n",
    "        '''\n",
    "        if categories is None:\n",
    "            categories = self.category_names()\n",
    "        # Make lists which are nonzero for labels in a category\n",
    "        catmap = {}\n",
    "        for cat in categories:\n",
    "            imap = self.category_index_map(cat)\n",
    "            if len(imap) < self.label_size(None):\n",
    "                imap = numpy.concatenate((imap, numpy.zeros(\n",
    "                    self.label_size(None) - len(imap), dtype=imap.dtype)))\n",
    "            catmap[cat] = imap\n",
    "        # For each label, find the category with maximum coverage.\n",
    "        result = []\n",
    "        for i in range(self.label_size(None)):\n",
    "            maxcov, maxcat = max(\n",
    "                    (self.coverage(cat, catmap[cat][i])\n",
    "                        if catmap[cat][i] else 0, ic)\n",
    "                    for ic, cat in enumerate(categories))\n",
    "            result.append(maxcat)\n",
    "        # Return the max-coverage cateogry for each label.\n",
    "        return numpy.array(result)\n",
    "    def segmentation_data(self, category, i, c=0, full=False, out=None):\n",
    "        '''\n",
    "        Returns a 2-d numpy matrix with segmentation data for the ith image,\n",
    "        restricted to the given category.  By default, maps all label numbers\n",
    "        to the category-specific dense mapping described in the c_*.csv\n",
    "        listing; but can be asked to expose the fully unique indexing by\n",
    "        using full=True.\n",
    "        '''\n",
    "        row = self.image[i]\n",
    "        data_channels = row.get(category, ())\n",
    "        if c >= len(data_channels):\n",
    "            channel = 0  # Deal with unlabeled data in this category\n",
    "        else:\n",
    "            channel = data_channels[c]\n",
    "        if out is None:\n",
    "            out = numpy.empty((row['sh'], row['sw']), dtype=numpy.int16)\n",
    "        if isinstance(channel, int):\n",
    "            if not full:\n",
    "                channel = self.category_map[category][channel]\n",
    "            out[:,:] = channel  # Single-label for the whole image\n",
    "            return out\n",
    "        png = imread(os.path.join(self.directory, 'images', channel))\n",
    "        if full:\n",
    "            # Full case: just combine png channels.\n",
    "            out[...] = png[:,:,0] + png[:,:,1] * 256\n",
    "        else:\n",
    "            # Dense case: combine png channels and apply the category map.\n",
    "            catmap = self.category_map[category]\n",
    "            out[...] = catmap[png[:,:,0] + png[:,:,1] * 256]\n",
    "        return out\n",
    "    \n",
    "    def full_segmentation_data(self, i,\n",
    "            categories=None, max_depth=None, out=None):\n",
    "        '''\n",
    "        Returns a 3-d numpy tensor with segmentation data for the ith image,\n",
    "        with multiple layers represnting multiple lables for each pixel.\n",
    "        The depth is variable depending on available data but can be\n",
    "        limited to max_depth.\n",
    "        '''\n",
    "        row = self.image[i]\n",
    "        if categories:\n",
    "            groups = [d for cat, d in row.items() if cat in categories and d]\n",
    "        else:\n",
    "            groups = [d for cat, d in row.items() if d and (\n",
    "                cat not in self.meta_categories)]\n",
    "        depth = sum(len(c) for c in groups)\n",
    "        if max_depth is not None:\n",
    "            depth = min(depth, max_depth)\n",
    "        # Allocate an array if not already allocated.\n",
    "        if out is None:\n",
    "            out = numpy.empty((depth, row['sh'], row['sw']), dtype=numpy.int16)\n",
    "        i = 0\n",
    "        # Stack up the result segmentation one channel at a time\n",
    "        for group in groups:\n",
    "            for channel in group:\n",
    "                if isinstance(channel, int):\n",
    "                    out[i] = channel\n",
    "                else:\n",
    "                    png = imread(\n",
    "                            os.path.join(self.directory, 'images', channel))\n",
    "                    out[i] = png[:,:,0] + png[:,:,1] * 256\n",
    "                i += 1\n",
    "                if i == depth:\n",
    "                    return out\n",
    "        # Return above when we get up to depth\n",
    "        assert False\n",
    "\n",
    "    def category_index_map(self, category):\n",
    "        return numpy.array(self.category_map[category])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. helper - setup_signit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_sigint():\n",
    "    import threading\n",
    "    if not isinstance(threading.current_thread(), threading._MainThread):\n",
    "        return None\n",
    "    return signal.signal(signal.SIGINT, signal.SIG_IGN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. helper - restore_sigint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_sigint(original):\n",
    "    import threading\n",
    "    if not isinstance(threading.current_thread(), threading._MainThread):\n",
    "        return\n",
    "    if original is None:\n",
    "        original = signal.SIG_DFL\n",
    "    signal.signal(signal.SIGINT, original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. load SegmentationPrefetcher class\n",
    "    HACK IT LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationPrefetcher:\n",
    "    '''\n",
    "    SegmentationPrefetcher will prefetch a bunch of segmentation\n",
    "    images using a multiprocessing pool, so you do not have to wait\n",
    "    around while the files get opened and decoded.  Just request\n",
    "    batches of images and segmentations calling fetch_batch().\n",
    "    '''\n",
    "    def __init__(self, segmentation, split=None, randomize=False,\n",
    "            segmentation_shape=None, categories=None, once=False,\n",
    "            start=None, end=None, batch_size=4, ahead=4, thread=False):\n",
    "        '''\n",
    "        Constructor arguments:\n",
    "        segmentation: The AbstractSegmentation to load.\n",
    "        split: None for no filtering, or 'train' or 'val' etc.\n",
    "        randomize: True to randomly shuffle order, or a random seed.\n",
    "        categories: a list of categories to include in each batch.\n",
    "        batch_size: number of data items for each batch.\n",
    "        ahead: the number of data items to prefetch ahead.\n",
    "        '''\n",
    "        self.segmentation = segmentation\n",
    "        self.split = split\n",
    "        self.randomize = randomize\n",
    "        self.random = random.Random()\n",
    "        if randomize is not True:\n",
    "            self.random.seed(randomize)\n",
    "        self.categories = categories\n",
    "        self.once = once\n",
    "        self.batch_size = batch_size\n",
    "        self.ahead = ahead\n",
    "        # Initialize the multiprocessing pool\n",
    "        n_procs = cpu_count()\n",
    "        if thread:\n",
    "            self.pool = ThreadPool(processes=n_procs)\n",
    "        else:\n",
    "            original_sigint_handler = setup_sigint()\n",
    "            self.pool = Pool(processes=n_procs, initializer=setup_sigint)\n",
    "            restore_sigint(original_sigint_handler)\n",
    "        # Prefilter the image indexes of interest\n",
    "        if start is None:\n",
    "            start = 0\n",
    "        if end is None:\n",
    "            end = segmentation.size()\n",
    "        self.indexes = range(start, end)\n",
    "        if split:\n",
    "            self.indexes = [i for i in self.indexes\n",
    "                    if segmentation.split(i) == split]\n",
    "        if self.randomize:\n",
    "            self.random.shuffle(self.indexes)\n",
    "        self.index = 0\n",
    "        self.result_queue = []\n",
    "        self.segmentation_shape = segmentation_shape\n",
    "        # Get dense catmaps\n",
    "        self.catmaps = [\n",
    "                segmentation.category_index_map(cat) if cat != 'image' else None\n",
    "                for cat in categories]\n",
    "\n",
    "    def next_job(self):\n",
    "        if self.index < 0:\n",
    "            return None\n",
    "        j = self.indexes[self.index]\n",
    "        result = (j,\n",
    "                self.segmentation.__class__,\n",
    "                self.segmentation.metadata(j),\n",
    "                self.segmentation.filename(j),\n",
    "                self.categories,\n",
    "                self.segmentation_shape)\n",
    "        self.index += 1\n",
    "        if self.index >= len(self.indexes):\n",
    "            if self.once:\n",
    "                self.index = -1\n",
    "            else:\n",
    "                self.index = 0\n",
    "                if self.randomize:\n",
    "                    # Reshuffle every time through\n",
    "                    self.random.shuffle(self.indexes)\n",
    "        return result\n",
    "\n",
    "    def batches(self):\n",
    "        '''Iterator for all batches'''\n",
    "        while True:\n",
    "            batch = self.fetch_batch()\n",
    "            if batch is None:\n",
    "                raise StopIteration\n",
    "            yield batch\n",
    "\n",
    "    def fetch_batch(self):\n",
    "        '''Returns a single batch as an array of dictionaries.'''\n",
    "        try:\n",
    "            self.refill_tasks()\n",
    "            if len(self.result_queue) == 0:\n",
    "                return None\n",
    "            result = self.result_queue.pop(0)\n",
    "            return result.get(31536000)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Caught KeyboardInterrupt, terminating workers\")\n",
    "            self.pool.terminate()\n",
    "            raise\n",
    "\n",
    "    def fetch_tensor_batch(self, bgr_mean=None, global_labels=False):\n",
    "        '''Iterator for batches as arrays of tensors.'''\n",
    "        batch = self.fetch_batch()\n",
    "        return self.form_caffe_tensors(batch, bgr_mean, global_labels)\n",
    "\n",
    "    def tensor_batches(self, bgr_mean=None, global_labels=False):\n",
    "        '''Returns a single batch as an array of tensors, one per category.'''\n",
    "        while True:\n",
    "            batch = self.fetch_tensor_batch(\n",
    "                    bgr_mean=bgr_mean, global_labels=global_labels)\n",
    "            if batch is None:\n",
    "                raise StopIteration\n",
    "            yield batch\n",
    "\n",
    "    def form_caffe_tensors(self, batch, bgr_mean=None, global_labels=False):\n",
    "        # Assemble a batch in [{'cat': data,..},..] format into\n",
    "        # an array of batch tensors, the first for the image, and the\n",
    "        # remaining for each category in self.categories, in order.\n",
    "        # This also applies a random flip if needed\n",
    "        if batch is None:\n",
    "            return None\n",
    "        batches = [[] for c in self.categories]\n",
    "        for record in batch:\n",
    "            default_shape = (1, record['sh'], record['sw'])\n",
    "            for c, cat in enumerate(self.categories):\n",
    "                if cat == 'image':\n",
    "                    # Normalize image with right RGB order and mean\n",
    "                    batches[c].append(normalize_image(\n",
    "                        record[cat], bgr_mean))\n",
    "                elif global_labels:\n",
    "                    batches[c].append(normalize_label(\n",
    "                        record[cat], default_shape, flatten=True))\n",
    "                else:\n",
    "                    catmap = self.catmaps[c]\n",
    "                    batches[c].append(catmap[normalize_label(\n",
    "                        record[cat], default_shape, flatten=True)])\n",
    "        return [numpy.concatenate(tuple(m[numpy.newaxis] for m in b))\n",
    "                for b in batches]\n",
    "\n",
    "    def refill_tasks(self):\n",
    "        # It will call the sequencer to ask for a sequence\n",
    "        # of batch_size jobs (indexes with categories)\n",
    "        # Then it will call pool.map_async\n",
    "        while len(self.result_queue) < self.ahead:\n",
    "            data = []\n",
    "            while len(data) < self.batch_size:\n",
    "                job = self.next_job()\n",
    "                if job is None:\n",
    "                    break\n",
    "                data.append(job)\n",
    "            if len(data) == 0:\n",
    "                return\n",
    "            self.result_queue.append(self.pool.map_async(prefetch_worker, data))\n",
    "\n",
    "    def close(self):\n",
    "        while len(self.result_queue):\n",
    "            result = self.result_queue.pop(0)\n",
    "            if result is not None:\n",
    "                result.wait(0.001)\n",
    "        self.pool.close()\n",
    "        self.poool.cancel_join_thread()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. helper - build_dense_label_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dense_label_array(label_data, key='number', allow_none=False):\n",
    "    '''\n",
    "    Input: set of rows with 'number' fields (or another field name key).\n",
    "    Output: array such that a[number] = the row with the given number.\n",
    "    '''\n",
    "    result = [None] * (max([d[key] for d in label_data]) + 1)\n",
    "    for d in label_data:\n",
    "        result[d[key]] = d\n",
    "    # Fill in none\n",
    "    if not allow_none:\n",
    "        example = label_data[0]\n",
    "        def make_empty(k):\n",
    "            return dict((c, k if c is key else type(v)())\n",
    "                    for c, v in example.items())\n",
    "        for i, d in enumerate(result):\n",
    "            if d is None:\n",
    "                result[i] = dict(make_empty(i))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. helper - loadmodel()\n",
    "    HACK IT LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmodel(hook_fn):\n",
    "    if MODEL_FILE is None:\n",
    "        model = torchvision.models.__dict__[MODEL](pretrained=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(MODEL_FILE)\n",
    "        if type(checkpoint).__name__ == 'OrderedDict' or type(checkpoint).__name__ == 'dict':\n",
    "            model = torchvision.models.__dict__[MODEL](num_classes=NUM_CLASSES)\n",
    "            if MODEL_PARALLEL:\n",
    "                state_dict = {str.replace(k, 'module.', ''): v for k, v in checkpoint[\n",
    "                    'state_dict'].items()}  # the data parallel layer will add 'module' before each layer name\n",
    "            else:\n",
    "                state_dict = checkpoint\n",
    "            model.load_state_dict(state_dict)\n",
    "        else:\n",
    "            model = checkpoint\n",
    "    for name in FEATURE_NAMES:\n",
    "        model._modules.get(name).register_forward_hook(hook_fn)\n",
    "    if GPU:\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. helper - prefetch_worker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefetch_worker(d):\n",
    "    if d is None:\n",
    "        return None\n",
    "    j, typ, m, fn, categories, segmentation_shape = d\n",
    "    segs, shape = typ.resolve_segmentation(m, categories=categories)\n",
    "    if segmentation_shape is not None:\n",
    "        for k, v in segs.items():\n",
    "            segs[k] = scale_segmentation(v, segmentation_shape)\n",
    "        shape = segmentation_shape\n",
    "    # Some additional metadata to provide\n",
    "    segs['sh'], segs['sw'] = shape\n",
    "    segs['i'] = j\n",
    "    segs['fn'] = fn\n",
    "    if categories is None or 'image' in categories:\n",
    "        segs['image'] = imread(fn)\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. helper - scale_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_segmentation(segmentation, dims, crop=False):\n",
    "    '''\n",
    "    Zooms a 2d or 3d segmentation to the given dims, using nearest neighbor.\n",
    "    '''\n",
    "    shape = numpy.shape(segmentation)\n",
    "    if len(shape) < 2 or shape[-2:] == dims:\n",
    "        return segmentation\n",
    "    peel = (len(shape) == 2)\n",
    "    if peel:\n",
    "        segmentation = segmentation[numpy.newaxis]\n",
    "    levels = segmentation.shape[0]\n",
    "    result = numpy.zeros((levels, ) + dims,\n",
    "            dtype=segmentation.dtype)\n",
    "    ratio = (1,) + tuple(res / float(orig)\n",
    "            for res, orig in zip(result.shape[1:], segmentation.shape[1:]))\n",
    "    if not crop:\n",
    "        safezoom(segmentation, ratio, output=result, order=0)\n",
    "    else:\n",
    "        ratio = max(ratio[1:])\n",
    "        height = int(round(dims[0] / ratio))\n",
    "        hmargin = (segmentation.shape[0] - height) // 2\n",
    "        width = int(round(dims[1] / ratio))\n",
    "        wmargin = (segmentation.shape[1] - height) // 2\n",
    "        safezoom(segmentation[:, hmargin:hmargin+height,\n",
    "            wmargin:wmargin+width],\n",
    "            (1, ratio, ratio), output=result, order=0)\n",
    "    if peel:\n",
    "        result = result[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. helper - safezoom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safezoom(array, ratio, output=None, order=0):\n",
    "    '''Like numpy.zoom, but does not crash when the first dimension\n",
    "    of the array is of size 1, as happens often with segmentations'''\n",
    "    dtype = array.dtype\n",
    "    if array.dtype == numpy.float16:\n",
    "        array = array.astype(numpy.float32)\n",
    "    if array.shape[0] == 1:\n",
    "        if output is not None:\n",
    "            output = output[0,...]\n",
    "        result = zoom(array[0,...], ratio[1:],\n",
    "                output=output, order=order)\n",
    "        if output is None:\n",
    "            output = result[numpy.newaxis]\n",
    "    else:\n",
    "        result = zoom(array, ratio, output=output, order=order)\n",
    "        if output is None:\n",
    "            output = result\n",
    "    return output.astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. helper - wants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wants(what, option):\n",
    "    if option is None:\n",
    "        return True\n",
    "    return what in option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. helper - normalize_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(rgb_image, bgr_mean):\n",
    "    \"\"\"\n",
    "    Load input image and preprocess for Caffe:\n",
    "    - cast to float\n",
    "    - switch channels RGB -> BGR\n",
    "    - subtract mean\n",
    "    - transpose to channel x height x width order\n",
    "    \"\"\"\n",
    "    img = numpy.array(rgb_image, dtype=numpy.float32)\n",
    "    if (img.ndim == 2):\n",
    "        img = numpy.repeat(img[:,:,None], 3, axis = 2)\n",
    "    img = img[:,:,::-1]\n",
    "    if bgr_mean is not None:\n",
    "        img -= bgr_mean\n",
    "    img = img.transpose((2,0,1))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. helper - normalize_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(label_data, shape, flatten=False):\n",
    "    \"\"\"\n",
    "    Given a 0, 1, 2, or 3-dimensional label_data and a default\n",
    "    shape of the form (1, y, x), returns a 3d tensor by \n",
    "    \"\"\"\n",
    "    dims = len(numpy.shape(label_data))\n",
    "    if dims <= 2:\n",
    "        # Scalar data on this channel: fill shape\n",
    "        if dims == 1:\n",
    "            if flatten:\n",
    "                label_data = label_data[0] if len(label_data) else 0\n",
    "            else:\n",
    "                return (numpy.ones(shape, dtype=numpy.int16) *\n",
    "                        numpy.asarray(label_data, dtype=numpy.int16)\n",
    "                            [:, numpy.newaxis, numpy.newaxis])\n",
    "        return numpy.full(shape, label_data, dtype=numpy.int16)\n",
    "    else:\n",
    "        if dims == 3:\n",
    "            if flatten:\n",
    "                label_data = label_data[0]\n",
    "            else:\n",
    "                return label_data\n",
    "        return label_data[numpy.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. Hook_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is this?\n",
    "features_blobs = []\n",
    "\n",
    "# update feature_blobs\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24. Load QuantileVector Class\n",
    "    Based on the optimal KLL quantile algorithm by Karnin, Lang, and Liberty\n",
    "    from FOCS 2016.  http://ieee-focs.org/FOCS-2016-Papers/3933a071.pdf\n",
    "    \n",
    "    HACK IT LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute quantiles using less memory\n",
    "class QuantileVector:\n",
    "    def __init__(self, depth=1, resolution=24 * 1024, buffersize=None,\n",
    "            dtype=None, seed=None):\n",
    "        self.resolution = resolution\n",
    "        self.depth = depth\n",
    "        # Default buffersize: 128 samples (and smaller than resolution).\n",
    "        if buffersize is None:\n",
    "            buffersize = min(128, (resolution + 7) // 8)\n",
    "        self.buffersize = buffersize\n",
    "        self.samplerate = 1.0\n",
    "        self.data = [numpy.zeros(shape=(depth, resolution), dtype=dtype)]\n",
    "        self.firstfree = [0]\n",
    "        self.random = numpy.random.RandomState(seed)\n",
    "        self.extremes = numpy.empty(shape=(depth, 2), dtype=dtype)\n",
    "        self.extremes.fill(numpy.NaN)\n",
    "        self.size = 0\n",
    "    \n",
    "    def add(self, incoming):\n",
    "        assert len(incoming.shape) == 2\n",
    "        assert incoming.shape[1] == self.depth\n",
    "        self.size += incoming.shape[0]\n",
    "        # Convert to a flat numpy array.\n",
    "        if self.samplerate >= 1.0:\n",
    "            self._add_every(incoming)\n",
    "            return\n",
    "        # If we are sampling, then subsample a large chunk at a time.\n",
    "        self._scan_extremes(incoming)\n",
    "        chunksize = numpy.ceil[self.buffersize / self.samplerate]\n",
    "        for index in range(0, len(incoming), chunksize):\n",
    "            batch = incoming[index:index+chunksize]\n",
    "            sample = batch[self.random.binomial(1, self.samplerate, len(batch))]\n",
    "            self._add_every(sample)\n",
    "        \n",
    "    def _add_every(self, incoming):\n",
    "        supplied = len(incoming)\n",
    "        index = 0\n",
    "        while index < supplied:\n",
    "            ff = self.firstfree[0]\n",
    "            available = self.data[0].shape[1] - ff\n",
    "            if available == 0:\n",
    "                if not self._shift():\n",
    "                    # If we shifted by subsampling, then subsample.\n",
    "                    incoming = incoming[index:]\n",
    "                    if self.samplerate >= 0.5:\n",
    "                        print('SAMPLING')\n",
    "                        self._scan_extremes(incoming)\n",
    "                    incoming = incoming[self.random.binomial(1, 0.5,\n",
    "                        len(incoming - index))]\n",
    "                    index = 0\n",
    "                    supplied = len(incoming)\n",
    "                ff = self.firstfree[0]\n",
    "                available = self.data[0].shape[1] - ff\n",
    "            copycount = min(available, supplied - index)\n",
    "            self.data[0][:,ff:ff + copycount] = numpy.transpose(\n",
    "                    incoming[index:index + copycount,:])\n",
    "            self.firstfree[0] += copycount\n",
    "            index += copycount\n",
    "\n",
    "    def _shift(self):\n",
    "        index = 0\n",
    "        # If remaining space at the current layer is less than half prev\n",
    "        # buffer size (rounding up), then we need to shift it up to ensure\n",
    "        # enough space for future shifting.\n",
    "        while self.data[index].shape[1] - self.firstfree[index] < (\n",
    "                -(-self.data[index-1].shape[1] // 2) if index else 1):\n",
    "            if index + 1 >= len(self.data):\n",
    "                return self._expand()\n",
    "            data = self.data[index][:,0:self.firstfree[index]]\n",
    "            data.sort()\n",
    "            if index == 0 and self.samplerate >= 1.0:\n",
    "                self._update_extremes(data[:,0], data[:,-1])\n",
    "            offset = self.random.binomial(1, 0.5)\n",
    "            position = self.firstfree[index + 1]\n",
    "            subset = data[:,offset::2]\n",
    "            self.data[index + 1][:,position:position + subset.shape[1]] = subset\n",
    "            self.firstfree[index] = 0\n",
    "            self.firstfree[index + 1] += subset.shape[1]\n",
    "            index += 1\n",
    "        return True\n",
    "\n",
    "    def _scan_extremes(self, incoming):\n",
    "        # When sampling, we need to scan every item still to get extremes\n",
    "        self._update_extremes(\n",
    "                numpy.nanmin(incoming, axis=0),\n",
    "                numpy.nanmax(incoming, axis=0))\n",
    "\n",
    "    def _update_extremes(self, minr, maxr):\n",
    "        self.extremes[:,0] = numpy.nanmin(\n",
    "                [self.extremes[:, 0], minr], axis=0)\n",
    "        self.extremes[:,-1] = numpy.nanmax(\n",
    "                [self.extremes[:, -1], maxr], axis=0)\n",
    "\n",
    "    def minmax(self):\n",
    "        if self.firstfree[0]:\n",
    "            self._scan_extremes(self.data[0][:,:self.firstfree[0]].transpose())\n",
    "        return self.extremes.copy()\n",
    "\n",
    "    def _expand(self):\n",
    "        cap = self._next_capacity()\n",
    "        if cap > 0:\n",
    "            # First, make a new layer of the proper capacity.\n",
    "            self.data.insert(0, numpy.empty(\n",
    "                shape=(self.depth, cap), dtype=self.data[-1].dtype))\n",
    "            self.firstfree.insert(0, 0)\n",
    "        else:\n",
    "            # Unless we're so big we are just subsampling.\n",
    "            assert self.firstfree[0] == 0\n",
    "            self.samplerate *= 0.5\n",
    "        for index in range(1, len(self.data)):\n",
    "            # Scan for existing data that needs to be moved down a level.\n",
    "            amount = self.firstfree[index]\n",
    "            if amount == 0:\n",
    "                continue\n",
    "            position = self.firstfree[index-1]\n",
    "            # Move data down if it would leave enough empty space there\n",
    "            # This is the key invariant: enough empty space to fit half\n",
    "            # of the previous level's buffer size (rounding up)\n",
    "            if self.data[index-1].shape[1] - (amount + position) >= (\n",
    "                    -(-self.data[index-2].shape[1] // 2) if (index-1) else 1):\n",
    "                self.data[index-1][:,position:position + amount] = (\n",
    "                        self.data[index][:,:amount])\n",
    "                self.firstfree[index-1] += amount\n",
    "                self.firstfree[index] = 0\n",
    "            else:\n",
    "                # Scrunch the data if it would not.\n",
    "                data = self.data[index][:,:amount]\n",
    "                data.sort()\n",
    "                if index == 1:\n",
    "                    self._update_extremes(data[:,0], data[:,-1])\n",
    "                offset = self.random.binomial(1, 0.5)\n",
    "                scrunched = data[:,offset::2]\n",
    "                self.data[index][:,:scrunched.shape[1]] = scrunched\n",
    "                self.firstfree[index] = scrunched.shape[1]\n",
    "        return cap > 0\n",
    "\n",
    "    def _next_capacity(self):\n",
    "        cap = numpy.ceil(self.resolution * numpy.power(0.67, len(self.data)))\n",
    "        if cap < 2:\n",
    "            return 0\n",
    "        return max(self.buffersize, int(cap))\n",
    "\n",
    "    def _weighted_summary(self, sort=True):\n",
    "        if self.firstfree[0]:\n",
    "            self._scan_extremes(self.data[0][:,:self.firstfree[0]].transpose())\n",
    "        size = sum(self.firstfree) + 2\n",
    "        weights = numpy.empty(\n",
    "            shape=(size), dtype='float32') # floating point\n",
    "        summary = numpy.empty(\n",
    "            shape=(self.depth, size), dtype=self.data[-1].dtype)\n",
    "        weights[0:2] = 0\n",
    "        summary[:,0:2] = self.extremes\n",
    "        index = 2\n",
    "        for level, ff in enumerate(self.firstfree):\n",
    "            if ff == 0:\n",
    "                continue\n",
    "            summary[:,index:index + ff] = self.data[level][:,:ff]\n",
    "            weights[index:index + ff] = numpy.power(2.0, level)\n",
    "            index += ff\n",
    "        assert index == summary.shape[1]\n",
    "        if sort:\n",
    "            order = numpy.argsort(summary)\n",
    "            summary = summary[numpy.arange(self.depth)[:,None], order]\n",
    "            weights = weights[order]\n",
    "        return (summary, weights)\n",
    "\n",
    "    def quantiles(self, quantiles, old_style=False):\n",
    "        if self.size == 0:\n",
    "            return numpy.full((self.depth, len(quantiles)), numpy.nan)\n",
    "        summary, weights = self._weighted_summary()\n",
    "        cumweights = numpy.cumsum(weights, axis=-1) - weights / 2\n",
    "        if old_style:\n",
    "            # To be convenient with numpy.percentile\n",
    "            cumweights -= cumweights[:,0:1]\n",
    "            cumweights /= cumweights[:,-1:]\n",
    "        else:\n",
    "            cumweights /= numpy.sum(weights, axis=-1, keepdims=True)\n",
    "        result = numpy.empty(shape=(self.depth, len(quantiles)))\n",
    "        for d in range(self.depth):\n",
    "            result[d] = numpy.interp(quantiles, cumweights[d], summary[d])\n",
    "        return result\n",
    "\n",
    "    def integrate(self, fun):\n",
    "        result = None\n",
    "        for level, ff in enumerate(self.firstfree):\n",
    "            if ff == 0:\n",
    "                continue\n",
    "            term = numpy.sum(\n",
    "                    fun(self.data[level][:,:ff]) * numpy.power(2.0, level),\n",
    "                    axis=-1)\n",
    "            if result is None:\n",
    "                result = term\n",
    "            else:\n",
    "                result += term\n",
    "        if result is not None:\n",
    "            result /= self.samplerate\n",
    "        return result\n",
    "\n",
    "    def percentiles(self, percentiles):\n",
    "        return self.quantiles(percentiles, old_style=True)\n",
    "\n",
    "    def readout(self, count, old_style=True):\n",
    "        return self.quantiles(\n",
    "                numpy.linspace(0.0, 1.0, count), old_style=old_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. load FeatureOperator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureOperator:\n",
    "    def __init__(self):\n",
    "            # make a new directory if the give path does not have an output folder\n",
    "            if not os.path.exists(OUTPUT_FOLDER): \n",
    "                os.makedirs(os.path.join(OUTPUT_FOLDER, 'image'))\n",
    "            # turn data into a SegmentationData class (see previous class definition) \n",
    "            self.data = SegmentationData(DATA_DIRECTORY, categories=CATAGORIES)\n",
    "            # prefetch some segmentation images using a multiprocessing pool\n",
    "            self.loader = SegmentationPrefetcher(self.data,categories=['image'],once=True,batch_size=BATCH_SIZE)\n",
    "            self.mean = [109.5388,118.6897,124.6901]\n",
    "            \n",
    "    def feature_extraction(self, model=None, memmap=True):\n",
    "        loader = self.loader\n",
    "        # extract the max value activaiton for each image\n",
    "        maxfeatures = [None] * len(FEATURE_NAMES) # len -> how many features? FEATURE_NAMES = [\"layer4\"]\n",
    "        wholefeatures = [None] * len(FEATURE_NAMES)\n",
    "        features_size = [None] * len(FEATURE_NAMES)\n",
    "        # get path of features_size_file\n",
    "        features_size_file = os.path.join(OUTPUT_FOLDER, \"feature_size.npy\")\n",
    "    \n",
    "        if memmap:\n",
    "            skip = True\n",
    "            # define paths for mmap_files\n",
    "            mmap_files =  [os.path.join(OUTPUT_FOLDER, \"%s.mmap\" % feature_name)  for feature_name in FEATURE_NAMES]\n",
    "            mmap_max_files = [os.path.join(OUTPUT_FOLDER, \"%s_max.mmap\" % feature_name) for feature_name in FEATURE_NAMES]\n",
    "            # if feature size file exists already\n",
    "            if os.path.exists(features_size_file):\n",
    "                # load the file -> something similar to a numpy dictionary\n",
    "                features_size = np.load(features_size_file)\n",
    "            # if feature size file does not exist\n",
    "            else:\n",
    "                skip = False\n",
    "            # note: zip (kind of) combine two iterables into tuples\n",
    "            for i, (mmap_file, mmap_max_file) in enumerate(zip(mmap_files,mmap_max_files)):\n",
    "                if os.path.exists(mmap_file) and os.path.exists(mmap_max_file) and features_size[i] is not None:\n",
    "                    print('loading features %s' % FEATURE_NAMES[i])\n",
    "                    # np.memmap: create a memory-map to an array stored in a binary file on disk.\n",
    "                    wholefeatures[i] = np.memmap(mmap_file, dtype=float,mode='r', shape=tuple(features_size[i]))\n",
    "                    maxfeatures[i] = np.memmap(mmap_max_file, dtype=float, mode='r', shape=tuple(features_size[i][:2]))\n",
    "                else:\n",
    "                    print('file missing, loading from scratch')\n",
    "                    skip = False\n",
    "        # skip is false if any of the files is missing\n",
    "            if skip:\n",
    "                return wholefeatures, maxfeatures\n",
    "        \n",
    "        # now run feature extraction no matter memmap is used or not\n",
    "        # loader.indexes & loader.batch_size comes from prefetcher class \n",
    "        # compute number of batches by divide the total number of samples by the batch size\n",
    "        num_batches = (len(loader.indexes) + loader.batch_size - 1) / loader.batch_size\n",
    "        # loader.tensor_batches is a batch of an array of tensors\n",
    "        for batch_idx,batch in enumerate(loader.tensor_batches(bgr_mean=self.mean)):\n",
    "            # del: remove an item in the list given an index; here empty the who list\n",
    "            del features_blobs[:]\n",
    "            input = batch[0]\n",
    "            batch_size = len(input)\n",
    "            print('extracting feature from batch %d / %d' % (batch_idx+1, num_batches))\n",
    "            #reverse the second intex \n",
    "            # note the second subarray is reversed\n",
    "            input = torch.from_numpy(input[:, ::-1, :, :].copy())\n",
    "            input.div_(255.0 * 0.224) # ?\n",
    "            # check if GPU is used\n",
    "            if GPU:\n",
    "                input = input.cuda()\n",
    "            # V = autograd.variable\n",
    "            input_var = V(input,volatile=True)  \n",
    "            # feed forward into the model?\n",
    "            logit = model.forward(input_var)\n",
    "            while np.isnan(logit.data.max()): # ok the coder doesn't know when this scenario will happen\n",
    "                print(\"nan\") \n",
    "                del features_blobs[:]\n",
    "                logit = model.forward(input_var)\n",
    "                \n",
    "            # initialize maxfeature variable\n",
    "            if maxfeatures[0] is None:\n",
    "                for i, feat_batch in enumerate(features_blobs):\n",
    "                    # I don't understand features_blobs\n",
    "                    size_features = (len(loader.indexes), feat_batch.shape[1])\n",
    "                    # if memmap, get the max feature\n",
    "                    if memmap:\n",
    "                         maxfeatures[i] = np.memmap(mmap_max_files[i],dtype=float,mode='w+',shape=size_features)\n",
    "                    # otherwise initialize with all zeros\n",
    "                    else:\n",
    "                        maxfeatures[i] = np.zeros(size_features)\n",
    "            \n",
    "            # what is feat_batch <- what is feature_blobs?\n",
    "            # initialize wholefeature variable\n",
    "            if len(feat_batch.shape) == 4 and wholefeatures[0] is None:\n",
    "                for i, feat_batch in enumerate(features_blobs):\n",
    "                    size_features = (len(loader.indexes), \n",
    "                                     feat_batch.shape[1], \n",
    "                                     feat_batch.shape[2], \n",
    "                                     feat_batch.shape[3])\n",
    "                    features_size[i] = size_features\n",
    "                    if memmap:\n",
    "                        wholefeatures[i] = np.memmap(mmap_files[i], dtype=float, mode='w+', shape=size_features)\n",
    "                    else:\n",
    "                        wholefeatures[i] = np.zeros(size_features)\n",
    "            \n",
    "            np.save(features_size_file, features_size)\n",
    "            # set start index and end index\n",
    "            start_idx = batch_idx*BATCH_SIZE\n",
    "            end_idx = min((batch_idx+1)*BATCH_SIZE, len(loader.indexes))\n",
    "            \n",
    "            # get feature variable\n",
    "            for i, feat_batch in enumerate(features_blobs):\n",
    "                if len(feat_batch.shape) == 4:\n",
    "                    wholefeatures[i][start_idx:end_idx] = feat_batch\n",
    "                    maxfeatures[i][start_idx:end_idx] = np.max(np.max(feat_batch,3),2)\n",
    "                elif len(feat_batch.shape) == 3:\n",
    "                    maxfeatures[i][start_idx:end_idx] = np.max(feat_batch, 2)\n",
    "                elif len(feat_batch.shape) == 2:\n",
    "                    maxfeatures[i][start_idx:end_idx] = feat_batch\n",
    "        \n",
    "        if len(feat_batch.shape) == 2:\n",
    "            wholefeatures = maxfeatures\n",
    "        return wholefeatures,maxfeatures\n",
    "    \n",
    "    def quantile_threshold(self, features, savepath=''):\n",
    "        # set path for quantile threshold\n",
    "        qtpath = os.path.join(OUTPUT_FOLDER, savepath)\n",
    "        # if the word has been done, simply return\n",
    "        if savepath and os.path.exists(qtpath):\n",
    "             return np.load(qtpath)\n",
    "        print(\"calculating quantile threshold\")\n",
    "        quant = QuantileVector(depth=features.shape[1], seed=1)\n",
    "        start_time = time.time()\n",
    "        last_batch_time = start_time\n",
    "        # setting batch size ???\n",
    "        batch_size = 64\n",
    "        for i in range(0, features.shape[0], batch_size):\n",
    "            # for each batch\n",
    "            # keep track of the time\n",
    "            batch_time = time.time()\n",
    "            rate = i / (batch_time - start_time + 1e-15)\n",
    "            batch_rate = batch_size / (batch_time - last_batch_time + 1e-15)\n",
    "            last_batch_time = batch_time\n",
    "            print('Processing quantile index %d: %f %f' % (i, rate, batch_rate))\n",
    "            # fetch elements in the batch\n",
    "            batch = features[i:i + batch_size]\n",
    "            batch = np.transpose(batch, axes=(0, 2, 3, 1)).reshape(-1, features.shape[1]) # fine\n",
    "            # Quantile Vector method: adding batch\n",
    "            quant.add(batch)\n",
    "        # QUANTILE = threshold for activation; = 0.005 by default\n",
    "        print(quant.readout(1000))\n",
    "        print(1-QUANTILE)\n",
    "        ret = quant.readout(1000)[:, int(1000 * (1-QUANTILE)-1)]\n",
    "        if savepath:\n",
    "            np.save(qtpath, ret)\n",
    "        return ret\n",
    "    \n",
    "    def tally_job(args):\n",
    "        features, data, threshold, tally_labels, tally_units, tally_units_cat, tally_both, start, end = args\n",
    "        # number of units\n",
    "        units = features.shape[1]\n",
    "        size_RF = (IMG_SIZE / features.shape[2], IMG_SIZE / features.shape[3])\n",
    "        fieldmap = ((0, 0), size_RF, size_RF)\n",
    "        pd = SegmentationPrefetcher(data, categories=data.category_names(),\n",
    "                                    once=True, batch_size=TALLY_BATCH_SIZE,\n",
    "                                    ahead=TALLY_AHEAD, start=start, end=end)\n",
    "        # keep track of time \n",
    "        count = start\n",
    "        start_time = time.time()\n",
    "        last_batch_time = start_time\n",
    "        for batch in pd.batches():\n",
    "            batch_time = time.time()\n",
    "            rate = (count - start) / (batch_time - start_time + 1e-15)\n",
    "            batch_rate = len(batch) / (batch_time - last_batch_time + 1e-15)\n",
    "            last_batch_time = batch_time\n",
    "            print('labelprobe image index %d, items per sec %.4f, %.4f' % (count, rate, batch_rate))\n",
    "            \n",
    "        for concept_map in batch:\n",
    "            count += 1\n",
    "            # get the index of the image\n",
    "            img_index = concept_map['i']\n",
    "            scalars, pixels = [], []\n",
    "            # for each category\n",
    "            for cat in data.category_names():\n",
    "                # get corresponding \n",
    "                label_group = concept_map[cat]\n",
    "                shape = np.shape(label_group)\n",
    "                if len(shape) % 2 == 0:\n",
    "                    label_group = [label_group]\n",
    "                if len(shape) < 2:\n",
    "                    scalars += label_group\n",
    "                else:\n",
    "                    pixels.append(label_group)\n",
    "                \n",
    "                for scalar in scalars:\n",
    "                    tally_labels[scalar] += concept_map['sh'] * concept_map['sw']\n",
    "                if pixels:\n",
    "                    pixels = np.concatenate(pixels)\n",
    "                    tally_label = np.bincount(pixels.ravel())\n",
    "                    if len(tally_label) > 0:\n",
    "                        tally_label[0] = 0\n",
    "                    tally_labels[:len(tally_label)] += tally_label\n",
    "            \n",
    "            # for each individual unit\n",
    "            for unit_id in range(units):\n",
    "                feature_map = features[img_index][unit_id]\n",
    "                # if max value exceed threshold for the unit\n",
    "                if feature_map.max() > threshold[unit_id]:\n",
    "                    # imresize? \n",
    "                    mask = imresize(feature_map, (concept_map['sh'], concept_map['sw']), mode='F')\n",
    "                    indexes = np.argwhere(mask > threshold[unit_id])\n",
    "                \n",
    "                    tally_units[unit_id] += len(indexes)\n",
    "                    if len(pixels) > 0:\n",
    "                        tally_bt = np.bincount(pixels[:, indexes[:, 0], indexes[:, 1]].ravel())\n",
    "                        if len(tally_bt) > 0:\n",
    "                            tally_bt[0] = 0\n",
    "                        tally_cat = np.dot(tally_bt[None,:], data.labelcat[:len(tally_bt), :])[0]\n",
    "                        tally_both[unit_id,:len(tally_bt)] += tally_bt\n",
    "                    for scalar in scalars:\n",
    "                        tally_cat += data.labelcat[scalar]\n",
    "                        tally_both[unit_id, scalar] += len(indexes)\n",
    "                    tally_units_cat[unit_id] += len(indexes) * (tally_cat > 0)\n",
    "\n",
    "                \n",
    "        \n",
    "    def tally(self, features, threshold, savepath=''):\n",
    "        csvpath = os.path.join(OUTPUT_FOLDER, savepath)\n",
    "        if savepath and os.path.exists(csvpath):\n",
    "            return load_csv(csvpath)\n",
    "        \n",
    "        # initialize \n",
    "        units = features.shape[1]\n",
    "        labels = len(self.data.label)\n",
    "        categories = self.data.category_names()\n",
    "        tally_both = np.zeros((units,labels),dtype=np.float64)\n",
    "        tally_units = np.zeros(units,dtype=np.float64)\n",
    "        tally_units_cat = np.zeros((units,len(categories)), dtype=np.float64)\n",
    "        tally_labels = np.zeros(labels,dtype=np.float64)\n",
    "        \n",
    "        # OK seems like something for parallel computing; check it later\n",
    "        if PARALLEL > 1:\n",
    "            psize = int(np.ceil(float(self.data.size()) / PARALLEL))\n",
    "            ranges = [(s, min(self.data.size(), s + psize)) for s in range(0, self.data.size(), psize) if\n",
    "                      s < self.data.size()]\n",
    "            params = [(features, self.data, threshold, tally_labels, tally_units, tally_units_cat, tally_both) + r for r in ranges]\n",
    "            threadpool = pool.ThreadPool(processes=PARALLEL)\n",
    "            threadpool.map(FeatureOperator.tally_job, params)\n",
    "        else:\n",
    "            FeatureOperator.tally_job((features, self.data, threshold, tally_labels, tally_units, tally_units_cat, tally_both, 0, self.data.size()))\n",
    "\n",
    "        primary_categories = self.data.primary_categories_per_index()\n",
    "        # compute dot product between \n",
    "        tally_units_cat = np.dot(tally_units_cat, self.data.labelcat.T)\n",
    "        # iou intersection over units\n",
    "        iou = tally_both / (tally_units_cat + tally_labels[np.newaxis,:] - tally_both + 1e-10)\n",
    "        pciou = np.array([iou * (primary_categories[np.arange(iou.shape[1])] == ci)[np.newaxis, :] for ci in range(len(self.data.category_names()))])\n",
    "        label_pciou = pciou.argmax(axis=2)\n",
    "        name_pciou = [\n",
    "            [self.data.name(None, j) for j in label_pciou[ci]]\n",
    "            for ci in range(len(label_pciou))]\n",
    "        score_pciou = pciou[\n",
    "            np.arange(pciou.shape[0])[:, np.newaxis],\n",
    "            np.arange(pciou.shape[1])[np.newaxis, :],\n",
    "            label_pciou]\n",
    "        bestcat_pciou = score_pciou.argsort(axis=0)[::-1]\n",
    "        ordering = score_pciou.max(axis=0).argsort()[::-1]\n",
    "        rets = [None] * len(ordering)\n",
    "        \n",
    "        for i,unit in enumerate(ordering):\n",
    "            bestcat = bestcat_pciou[0, unit]\n",
    "            data = {\n",
    "                'unit': (unit + 1),\n",
    "                'category': categories[bestcat],\n",
    "                'label': name_pciou[bestcat][unit],\n",
    "                'score': score_pciou[bestcat][unit]\n",
    "            }\n",
    "            for ci, cat in enumerate(categories):\n",
    "                label = label_pciou[ci][unit]\n",
    "                data.update({\n",
    "                    '%s-label' % cat: name_pciou[ci][unit],\n",
    "                    '%s-truth' % cat: tally_labels[label],\n",
    "                    '%s-activation' % cat: tally_units_cat[unit, label],\n",
    "                    '%s-intersect' % cat: tally_both[unit, label],\n",
    "                    '%s-iou' % cat: score_pciou[ci][unit]\n",
    "                })\n",
    "            rets[i] = data\n",
    "\n",
    "        if savepath:\n",
    "            import csv\n",
    "            csv_fields = sum([[\n",
    "                '%s-label' % cat,\n",
    "                '%s-truth' % cat,\n",
    "                '%s-activation' % cat,\n",
    "                '%s-intersect' % cat,\n",
    "                '%s-iou' % cat] for cat in categories],\n",
    "                ['unit', 'category', 'label', 'score'])\n",
    "            with open(csvpath, 'w') as f:\n",
    "                writer = csv.DictWriter(f, csv_fields)\n",
    "                writer.writeheader()\n",
    "                for i in range(len(ordering)):\n",
    "                    writer.writerow(rets[i])\n",
    "        return rets\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. Define a Feature_Operator variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = FeatureOperator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  28. Do feature extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading features layer4\n"
     ]
    }
   ],
   "source": [
    "model = loadmodel(hook_feature)\n",
    "features, maxfeature = fo.feature_extraction(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check features and maxfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63305, 512, 7, 7)\n",
      "(63305, 512)\n",
      "[[[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    6.18270859e-02 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [4.22285572e-02 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [6.83312476e-01 7.06506908e-01 4.65398490e-01 ... 4.30700123e-01\n",
      "    4.63653445e-01 4.54222798e-01]\n",
      "   [1.08181641e-01 3.98174137e-01 9.09482956e-01 ... 9.84751582e-01\n",
      "    7.56086648e-01 4.43169713e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 2.98033744e-01 ... 8.27139556e-01\n",
      "    5.56976974e-01 2.14980170e-01]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[3.58171910e-01 6.54309273e-01 4.05489057e-01 ... 0.00000000e+00\n",
      "    3.48190874e-01 1.29773930e-01]\n",
      "   [3.52273196e-01 3.56225580e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    5.49530447e-01 4.62706268e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    2.67205387e-01 3.89223248e-01]\n",
      "   ...\n",
      "   [1.03909820e-02 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 3.06733668e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 3.68812770e-01]\n",
      "   [1.17052115e-01 2.03420490e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 2.41953313e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 5.72190523e-01 7.48847961e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [4.50028300e-01 1.26078200e+00 1.45922387e+00 ... 2.51881421e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.01420665e+00 1.36486650e+00 1.34352911e+00 ... 1.02959478e+00\n",
      "    6.95536017e-01 1.84379399e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 8.72196913e-01 ... 2.07357097e+00\n",
      "    1.35529184e+00 3.54169458e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 1.04051745e+00 ... 4.19723940e+00\n",
      "    2.83734226e+00 1.04641283e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 1.71584755e-01 ... 4.04103756e+00\n",
      "    2.85354710e+00 9.00121689e-01]]\n",
      "\n",
      "  [[7.27480510e-04 0.00000000e+00 8.00671354e-02 ... 0.00000000e+00\n",
      "    3.19498144e-02 0.00000000e+00]\n",
      "   [9.74345878e-02 1.41328692e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 1.47942407e-02]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.67500123e-01 1.18732631e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[3.53322119e-01 3.94327283e-01 3.74271899e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [9.11773622e-01 1.11626589e+00 1.04691613e+00 ... 6.01191521e-01\n",
      "    5.43832123e-01 6.10206425e-01]\n",
      "   [1.47845864e+00 2.01840901e+00 1.75153565e+00 ... 1.09848797e+00\n",
      "    1.09753609e+00 1.13823831e+00]\n",
      "   ...\n",
      "   [1.53352654e+00 2.92245078e+00 2.67138624e+00 ... 1.43350971e+00\n",
      "    7.75198877e-01 1.04474819e+00]\n",
      "   [1.82411754e+00 4.05456734e+00 3.31918812e+00 ... 1.32929921e+00\n",
      "    3.41180116e-01 3.58581156e-01]\n",
      "   [1.18329895e+00 1.88038123e+00 1.94475782e+00 ... 8.04388762e-01\n",
      "    3.46843340e-02 0.00000000e+00]]\n",
      "\n",
      "  [[6.46338105e-01 4.81467992e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.79760540e+00 1.66149831e+00 4.56461340e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.74315023e+00 1.66358101e+00 4.34200257e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [9.68938649e-01 1.31874990e+00 1.20072424e+00 ... 8.52572560e-01\n",
      "    6.42465293e-01 6.37815356e-01]\n",
      "   [4.71996784e-01 5.41225374e-01 1.96724698e-01 ... 5.85902222e-02\n",
      "    1.39255688e-01 0.00000000e+00]\n",
      "   [4.64592651e-02 4.97299507e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.20326184e-01 6.18484795e-01 3.83645177e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [1.04966924e-01 3.36028934e-01 7.75068700e-01 ... 7.19743013e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [2.58221358e-01 2.58009821e-01 3.94252419e-01 ... 2.22398311e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.85660082e-01 4.41340536e-01 3.37114632e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[7.18118310e-01 7.51415253e-01 3.66372794e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [6.39207959e-01 5.48662722e-01 2.10425079e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    2.87841201e-01 2.53341198e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.66749263e-01\n",
      "    4.04593498e-01 5.14791250e-01]]\n",
      "\n",
      "  [[1.06121147e+00 1.14669168e+00 5.27995467e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [7.88763762e-01 6.89957321e-01 1.97777718e-01 ... 4.26048338e-02\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.41308832e+00 1.15855753e+00 7.53837526e-01 ... 1.40305591e+00\n",
      "    1.10610998e+00 4.14011866e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 3.33318561e-01\n",
      "    9.43591177e-01 6.83527172e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    3.69878680e-01 3.20043504e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    2.74461210e-01 6.62990436e-02]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 2.08191693e-01 ... 2.65930355e-01\n",
      "    5.15962303e-01 3.75079155e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 4.32635486e-01\n",
      "    9.59838152e-01 6.68065071e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 4.98634934e-01\n",
      "    1.14572239e+00 7.16128170e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 8.27746630e-01\n",
      "    1.15681434e+00 6.61054015e-01]]]\n",
      "\n",
      "\n",
      " [[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [2.45617360e-01 7.59336352e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [2.53369600e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 3.56352031e-01 5.35875201e-01 ... 2.64938325e-01\n",
      "    1.61412075e-01 0.00000000e+00]\n",
      "   [0.00000000e+00 7.62296990e-02 2.89898008e-01 ... 5.44497728e-01\n",
      "    2.20998481e-01 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.70647444e-02\n",
      "    1.07503375e-02 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 3.92158747e-01 ... 5.89189053e-01\n",
      "    2.42020920e-01 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 4.65184331e-01 ... 1.26205564e+00\n",
      "    9.58233058e-01 4.18983340e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    1.58288285e-01 2.44572461e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [2.28051439e-01 5.79001382e-02 0.00000000e+00 ... 3.10897902e-02\n",
      "    4.49573338e-01 5.26852965e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    4.85465437e-01 3.92039806e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    2.29949966e-01 1.46471083e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [1.27428696e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.55781895e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.30219877e-01 2.27028370e-01 1.89932704e-01 ... 5.58062136e-01\n",
      "    9.02286649e-01 1.85614288e-01]]\n",
      "\n",
      "  [[6.31296873e-01 3.61339971e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.58201694e-01 4.09987032e-01 8.60002860e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 9.06551853e-02 3.45019937e-01 ... 4.69620287e-01\n",
      "    7.97263503e-01 3.13982457e-01]\n",
      "   [1.26907766e-01 0.00000000e+00 1.13928355e-01 ... 2.87976146e-01\n",
      "    4.23289299e-01 2.38328651e-01]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.01337492e-01 0.00000000e+00 1.26559377e-01 ... 4.73338962e-01\n",
      "    6.67192757e-01 5.82959056e-01]\n",
      "   [3.24748755e-01 6.02618277e-01 1.18448734e+00 ... 1.88337111e+00\n",
      "    2.05217147e+00 1.24328184e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 6.93319023e-01 1.90566146e+00 ... 1.55950570e+00\n",
      "    5.29324472e-01 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 1.21146309e+00 ... 6.17607236e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 3.16186816e-01 ... 2.62994617e-01\n",
      "    0.00000000e+00 0.00000000e+00]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [2.54295766e-01 2.87346821e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.13395333e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 1.20574259e-03]\n",
      "   [2.82468200e-02 1.23161264e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    3.15822661e-01 3.07273477e-01]]\n",
      "\n",
      "  [[0.00000000e+00 6.37527462e-03 3.56508791e-01 ... 1.08884042e-02\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 4.37623233e-01 1.45154679e+00 ... 5.55363894e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 9.12275553e-01 ... 7.54699945e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.29253888e+00 1.08397269e+00 4.08691943e-01 ... 5.96905053e-01\n",
      "    1.01291358e+00 1.01372039e+00]\n",
      "   [2.00658607e+00 1.82577133e+00 7.32080817e-01 ... 1.00072110e+00\n",
      "    1.99902189e+00 2.58386874e+00]\n",
      "   [2.81434941e+00 3.44413471e+00 1.78603220e+00 ... 1.91371214e+00\n",
      "    3.03093004e+00 3.31409359e+00]\n",
      "   ...\n",
      "   [6.63467646e+00 9.09855080e+00 7.97395706e+00 ... 7.37545443e+00\n",
      "    8.67805099e+00 6.90368986e+00]\n",
      "   [7.52719212e+00 9.00141716e+00 7.09958124e+00 ... 7.58317995e+00\n",
      "    8.79328537e+00 7.63133335e+00]\n",
      "   [5.60866547e+00 5.75392389e+00 4.35654545e+00 ... 4.23977041e+00\n",
      "    5.61066246e+00 5.25785589e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 5.96372306e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [1.72082677e-01 2.26585895e-01 3.77162784e-01 ... 9.45464134e-01\n",
      "    3.05563629e-01 3.58281225e-01]\n",
      "   [2.20019650e+00 1.82441437e+00 8.93704712e-01 ... 8.62840891e-01\n",
      "    1.17824423e+00 1.91162133e+00]\n",
      "   [2.28382254e+00 2.03259444e+00 1.45085573e+00 ... 1.35868585e+00\n",
      "    1.58796084e+00 1.70872009e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[2.13228416e+00 1.10050952e+00 2.54817307e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.45916247e+00 2.19442916e+00 1.35674000e+00 ... 4.01445776e-01\n",
      "    5.82033694e-01 6.54027045e-01]\n",
      "   [1.59908843e+00 1.91783214e+00 1.81404924e+00 ... 1.21266711e+00\n",
      "    1.47422624e+00 2.17449021e+00]\n",
      "   ...\n",
      "   [2.37436026e-01 5.89297056e-01 1.30722940e+00 ... 9.90177155e-01\n",
      "    6.98542893e-01 1.13195848e+00]\n",
      "   [2.61024907e-02 3.33030969e-02 2.20380753e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[7.18734384e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.48620337e-01\n",
      "    1.73472345e-01 8.70743155e-01]\n",
      "   [8.49844515e-02 1.04279242e-01 1.78993076e-01 ... 4.44951773e-01\n",
      "    4.72318381e-01 9.13061261e-01]\n",
      "   [0.00000000e+00 7.22066313e-02 2.45182976e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 2.31140926e-01 8.11405659e-01 ... 1.16659689e+00\n",
      "    9.55146134e-01 6.17284000e-01]\n",
      "   [3.59397233e-01 5.10395169e-01 7.83153117e-01 ... 4.89223778e-01\n",
      "    6.51828885e-01 5.06244898e-01]\n",
      "   [3.81626218e-04 0.00000000e+00 4.71723676e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[2.80222320e-03 5.13442233e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [2.86691010e-01 7.74846137e-01 9.36167836e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [4.69272286e-01 1.12184179e+00 1.68887818e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [5.88937104e-02 4.81117249e-01 9.52319801e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 7.30712414e-01 1.82488695e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[5.82529783e-01 1.64910471e+00 9.19022262e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [7.55428195e-01 1.03426552e+00 8.71004939e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [3.91074300e-01 5.34652412e-01 4.39077795e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [2.10926170e-03 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [5.73912561e-01 6.34993374e-01 3.58511150e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [6.43897712e-01 1.57690620e+00 5.09328663e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [4.21809554e-02 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [5.84857166e-01 4.91686672e-01 2.63611823e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [3.95756662e-01 3.89540404e-01 4.14214224e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[3.03840768e-02 0.00000000e+00 0.00000000e+00 ... 1.60874754e-01\n",
      "    2.74498791e-01 1.00499339e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [2.35373616e-01 2.64107496e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [9.10108268e-01 1.10322714e+00 1.97229356e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [6.14101589e-01 7.47930408e-01 3.78729463e-01 ... 1.71595495e-02\n",
      "    2.96511889e-01 3.98796111e-01]\n",
      "   [2.23176435e-01 1.30346358e-01 0.00000000e+00 ... 1.62902866e-02\n",
      "    1.51632607e-01 1.86788812e-01]]\n",
      "\n",
      "  [[0.00000000e+00 1.90113455e-01 8.86734903e-01 ... 7.23354161e-01\n",
      "    2.09754601e-01 0.00000000e+00]\n",
      "   [0.00000000e+00 1.33348644e-01 5.52190065e-01 ... 5.64049840e-01\n",
      "    1.35136113e-01 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 2.31449500e-01 ... 5.69922328e-01\n",
      "    5.97975791e-01 1.35968924e-01]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 1.20728850e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.99854717e-01 6.73663795e-01 1.17942226e+00 ... 7.06423283e-01\n",
      "    3.34658474e-01 1.33369401e-01]\n",
      "   [1.05261005e-01 2.84821332e-01 4.54849750e-01 ... 7.21567333e-01\n",
      "    8.28085303e-01 6.22985065e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 6.57176673e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 2.97844052e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 2.20300108e-02]\n",
      "   ...\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 2.41339535e-01 3.30941409e-01 ... 1.08805597e-01\n",
      "    4.55654934e-02 1.39391283e-02]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 7.40009248e-02 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [2.56531388e-01 3.52997243e-01 2.71209121e-01 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   ...\n",
      "   [4.40811485e-01 1.02491927e+00 1.03307557e+00 ... 1.02617395e+00\n",
      "    1.05931354e+00 6.59590662e-01]\n",
      "   [0.00000000e+00 1.39477074e-01 6.56831264e-01 ... 1.04313946e+00\n",
      "    8.03612769e-01 4.07688022e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[2.90725291e-01 2.64937222e-01 5.10699928e-01 ... 6.08334124e-01\n",
      "    1.60255358e-01 0.00000000e+00]\n",
      "   [1.52286053e+00 1.62266982e+00 1.10639620e+00 ... 4.05416906e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.39773238e+00 1.32146585e+00 1.03384352e+00 ... 5.10722399e-01\n",
      "    3.80425632e-01 3.32577318e-01]\n",
      "   ...\n",
      "   [3.15654725e-01 6.76487014e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "    1.16536608e-02 0.00000000e+00]]]]\n"
     ]
    }
   ],
   "source": [
    "print(features[0].shape)\n",
    "print(maxfeature[0].shape)\n",
    "print(features[0][1 : 1 + 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 29. Calculate threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 layer4\n"
     ]
    }
   ],
   "source": [
    "# want to have P(a_k > T_k) = 0.005, where k is a unit\n",
    "for layer_id,layer in enumerate(FEATURE_NAMES):\n",
    "    print(layer_id, layer)\n",
    "    thresholds = fo.quantile_threshold(features[layer_id],savepath=\"quantile.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.43853621  4.55046436  4.06314629  4.12000149  5.05443461  4.79213922\n",
      "  4.39332211  3.47959272  4.54271092  4.49178022  4.0640447   3.78068728\n",
      "  3.98791543  3.81424258  5.02142644  5.61813864  4.29361864  4.72877865\n",
      "  4.44804217  5.39878198  4.09035414  3.71769504  4.69778145  5.54218417\n",
      "  6.56921905  5.19914774  3.85738841  3.1261091   4.92014316  5.37708816\n",
      "  5.73575498  5.09114827  4.29013707  3.80249471  5.80139296  4.33226728\n",
      "  4.52199594  4.7900449   4.01003632  4.48949969  3.98607345  4.41653172\n",
      " 10.92450669  4.72149183  4.87549684  3.76088214  4.12702826  4.2582029\n",
      "  5.71867541  4.72855389  3.61776203  4.99382964  5.40668213  4.91268551\n",
      "  4.59592198  3.68096597  6.20649183  4.5349661   4.43002131  4.16518268\n",
      "  5.40909306  4.04767945  4.99100617  4.20542415  4.53713373  3.86571654\n",
      "  4.80346708  4.10799003  4.6062381   4.6786065   5.69957591  4.00954875\n",
      "  4.22179444  4.60000252  4.26311627  4.27468306  4.53506481  3.62456069\n",
      "  4.22296945  3.96273111  4.70673711  3.46838566  3.65908313  4.09386357\n",
      "  4.48918587  4.24765546  3.73218415  4.6416015   4.57888989  3.92500877\n",
      "  3.90676279  4.3117978   4.26809998  4.42001471  4.09295324  5.47854585\n",
      "  4.44414852  3.90873974  4.11448143  4.57384584  5.50600949  5.38683487\n",
      "  5.1156599   4.66300055  4.35532518  3.20487568  4.06602335  4.90428389\n",
      "  4.59964973  4.86495772  4.07528852  3.74542433  7.54794064  3.83573401\n",
      "  5.16301022  3.96719658  3.78009598  4.90648699  3.51231095  4.63623591\n",
      "  4.30169059  4.47444086  6.07797203  4.02250484  7.05803047  7.33734163\n",
      "  5.06357333  4.518525    4.25413363  4.12535525  4.53348502  4.20994798\n",
      "  4.37524749  6.41076674  3.64050407  4.16489902  5.70627782  4.47140748\n",
      "  4.03890026  4.58821173  5.35528559  3.95266891  3.62098299  4.97395377\n",
      "  3.91212287  4.60151237  4.23444753  3.84070878  4.32375758  4.00552859\n",
      "  3.64625048  3.67364851  3.84642565  6.35525411  4.20238926  4.78719346\n",
      "  5.27718462  4.67380436  4.34440389  5.02153077  3.69800743  4.13715917\n",
      "  4.99785246  4.17789881  4.20196527  4.96005776  4.10842345  4.49594422\n",
      "  4.13192558  4.75965859  4.76619247  4.61028366  4.47476631  3.65851774\n",
      "  3.75933277  4.35446353  3.99302685  4.20963967  3.95934091  4.33198665\n",
      "  6.14236038  4.35114115  4.20768033  4.9459051   5.0595791   4.29925417\n",
      "  3.87437517  4.85363262  3.73075318  5.22394566  3.97276954  4.12065155\n",
      "  4.03552252  3.73773712  3.58427446  4.16026982  4.1130035   4.87483905\n",
      "  4.26117025  5.28376144  4.05987415  4.18796739  3.94678179  6.77436197\n",
      "  4.42035638  3.39962441  3.95125675  4.95677087  4.40184519  5.70950007\n",
      "  4.08290344  4.50826228  3.75636908  4.09794625  4.27513046  4.4981315\n",
      "  5.07932653  4.41587496  4.54754476  6.08447073  4.02649095  5.83892728\n",
      "  4.16983804  4.14722411  4.98684118  6.08243056  4.89534932  4.13879754\n",
      "  4.34846812  4.47723279  4.06546704  4.0296619   3.93854874  4.15604578\n",
      "  6.16558664  3.95248893  3.53370107  4.40963121  4.46372075  3.85557165\n",
      "  4.17628526  4.34165075  5.36475132  6.5807024   5.57236871  4.11975993\n",
      "  4.64689938  7.29740599  4.52086384  3.79292199  4.4803064   5.42721432\n",
      "  5.08301385  4.00159428  4.44163054  3.98197779  4.02982368  4.07456563\n",
      "  3.63751822  5.37666002  3.89445652  4.75347108  4.18269521  5.3103113\n",
      "  4.26909464  4.71019501  4.13149176  4.00298033  4.236844    4.42773332\n",
      "  4.43167525  4.05386368  4.57235706  4.07938485  4.40583861  4.45829942\n",
      "  5.16446348  3.92795855  4.13483008  3.66598185  5.62068645  4.19160881\n",
      "  4.79447349  4.99888764  5.16301885  4.93596262  4.25096058  3.75695904\n",
      "  5.95857924  4.05071823  4.14055609  8.64469754  4.55665631  4.73801747\n",
      "  3.97423037  4.7215502   4.2075459   5.76250892  4.99070951  4.55905947\n",
      "  4.03027017  4.11559183  4.7112306   5.25231385  3.80525862  5.75214985\n",
      "  4.21579787  3.69934419  5.29943808  4.20186546  8.9519235   3.74289915\n",
      "  3.94917353  4.44032465  4.18966179  3.67486656  4.09281939  5.81800963\n",
      "  6.6206115   4.35807226  3.52934498  4.28818134  4.44211811  3.5939904\n",
      "  4.11807177  5.24080779  6.45564029  4.65877737  4.91179897  3.92237931\n",
      "  4.39935057  4.91212826  3.87938566  3.91218396  3.82071633  4.67353791\n",
      "  4.60781517  4.67426038  3.95236802  3.79965866  5.80182331  4.74222877\n",
      "  4.39139536  5.93474652  4.49486553  4.6898644   3.52158721  3.62863453\n",
      "  3.88742644  4.1997264   6.27907629  3.68678421  4.02297795  4.76411173\n",
      "  3.88228896  4.14086784  4.81336252  4.61759986  3.9807248   4.59541944\n",
      "  3.92000667  4.02709446  3.96512057  4.31805196  3.99012563  4.151665\n",
      "  3.83759574  4.00803645  4.38587838  4.57478542  3.155145    5.9282505\n",
      "  4.51771729  5.38000822  4.56514207  4.21203607  4.50462352  5.17818644\n",
      "  4.16806749  4.17682634  4.99991745  3.7986387   5.54502191  5.29589693\n",
      "  5.08132455  3.72486522  3.73154275  4.32619975  6.85756958  3.99840068\n",
      "  4.1459726   4.25137966  4.13483396  4.31016754  5.44035371  3.92148885\n",
      "  4.4390054   5.60876537  3.7559056   5.29334817  6.23793223  4.01996349\n",
      "  4.52153465  3.74071346  3.92365115  3.95713562  3.8421037   4.38652345\n",
      "  4.97939841  4.77239115  3.87077708  5.30978099  5.05152234  3.9986053\n",
      "  4.44606747  4.08211603  3.89966936  4.45482356  7.50841868  6.26547685\n",
      "  6.70250423  5.06944531  4.32822599  3.74826323  3.70882882  4.77048926\n",
      "  4.19951362  3.95443173  4.68833081  5.03446395  4.92424818  4.70802443\n",
      "  4.29233269  4.85012568  4.72595139  5.24022808  4.48164138  4.56409563\n",
      "  4.60690191  4.57662795  4.04076642  3.9502427   5.43121212  4.88519987\n",
      "  5.33751047  4.88284201  5.33265759  3.92791944  3.78343036  4.4366359\n",
      "  4.02113881  4.1387937   4.25624493  6.04370584  3.96918789  4.47486073\n",
      "  7.70692561  4.05453071  3.53114598  3.72297545  4.4546073   3.92457092\n",
      "  4.21881197  4.6225493   5.09466929  5.89943468  3.46660846  5.09805322\n",
      "  4.96297392  4.66198841  3.83899366  4.11841758  4.46976267  4.65292876\n",
      "  4.5219087   4.04060658  5.41075051  5.70106471  4.16192286  4.77234919\n",
      "  3.8945219   4.51663171  4.5349735   6.21898117  5.03886173  4.14736154\n",
      "  4.29555452  3.43086799  3.8500902   4.89377467  5.31599544  4.02016277\n",
      "  3.53278358  4.76090317  4.23779646  4.12843663  4.70336719  4.36597183\n",
      "  5.2927499   6.77018205  3.98469828  4.39617993  4.27147489  6.30259881\n",
      "  3.86496534  4.72967236  3.64275304  4.40161883  6.66074082  3.84728288\n",
      "  4.08019012  4.26264532]\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(thresholds)\n",
    "print(len(thresholds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30. Do Tally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 layer4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/ipykernel/__main__.py:128: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../project/rcc/deep_learning_hack/netdissect/broden1_224/images/a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-25-53e4519dd84e>\", line 5, in prefetch_worker\n    segs, shape = typ.resolve_segmentation(m, categories=categories)\n  File \"<ipython-input-14-81954b88f468>\", line 128, in resolve_segmentation\n    rgb = imread(os.path.join(directory, 'images', channel))\n  File \"/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/site-packages/numpy/lib/utils.py\", line 101, in newfunc\n    return func(*args, **kwds)\n  File \"/home/canliu/.local/lib/python3.6/site-packages/scipy/misc/pilutil.py\", line 164, in imread\n    im = Image.open(name)\n  File \"/home/canliu/.local/lib/python3.6/site-packages/PIL/Image.py\", line 2548, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '../../../project/rcc/deep_learning_hack/netdissect/broden1_224/images/a'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-2e81d0813979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFEATURE_NAMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtally_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msavepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tally.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-4ec062a47d2f>\u001b[0m in \u001b[0;36mtally\u001b[0;34m(self, features, threshold, savepath)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mthreadpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeatureOperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtally_job\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mFeatureOperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtally_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtally_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtally_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtally_units_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtally_both\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mprimary_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_categories_per_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-4ec062a47d2f>\u001b[0m in \u001b[0;36mtally_job\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mlast_batch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-510371ec19d9>\u001b[0m in \u001b[0;36mbatches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;34m'''Iterator for all batches'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-510371ec19d9>\u001b[0m in \u001b[0;36mfetch_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m31536000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Caught KeyboardInterrupt, terminating workers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/Anaconda3-5.0.1-el7-x86_64/envs/DL_GPU/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../project/rcc/deep_learning_hack/netdissect/broden1_224/images/a'"
     ]
    }
   ],
   "source": [
    "for layer_id, layer in enumerate(FEATURE_NAMES):\n",
    "    print(layer_id, layer)\n",
    "    tally_result = fo.tally(features[layer_id],thresholds,savepath=\"tally.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename, readfields=None):\n",
    "    def convert(value):\n",
    "        if re.match(r'^-?\\d+$', value): # ok regular expression stuffs\n",
    "            try:\n",
    "                return int(value)\n",
    "            except:\n",
    "                pass\n",
    "        if re.match(r'^-?[\\.\\d]+(?:e[+=]\\d+)$', value):\n",
    "            try:\n",
    "                return float(value)\n",
    "            except:\n",
    "                pass\n",
    "        return value\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        result = [{k: convert(v) for k, v in row.items()} for row in reader]\n",
    "        if readfields is not None:\n",
    "            readfields.extend(reader.fieldnames)\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL_GPU]",
   "language": "python",
   "name": "conda-env-DL_GPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
